{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.  Accuracy Measurement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries, load submissions and comments with summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24719/1047924690.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  nan_rows_df = nan_rows_df.append(extract_nan_rows(df, \"tfcc_submissions_top20_with_sentiment_including_comment_sentiment_and_summaries\"))\n",
      "/tmp/ipykernel_24719/1047924690.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  nan_rows_df = nan_rows_df.append(extract_nan_rows(comments_df, \"tfcc_top_comments_summarized\"))\n",
      "/tmp/ipykernel_24719/1047924690.py:20: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  nan_rows_df = nan_rows_df.append(extract_nan_rows(pegasus_df, \"tfcc_submissions_top20_pegasus_summaries\"))\n",
      "/tmp/ipykernel_24719/1047924690.py:21: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  nan_rows_df = nan_rows_df.append(extract_nan_rows(pegasus_comments_df, \"tfcc_top_comments_pegasus_summarized\"))\n",
      "/tmp/ipykernel_24719/1047924690.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nan_rows['source'] = name\n",
      "/tmp/ipykernel_24719/1047924690.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  nan_rows_df = nan_rows_df.append(extract_nan_rows(cohere_df, \"tfcc_submissions_top20_cohere_summaries\"))\n",
      "/tmp/ipykernel_24719/1047924690.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nan_rows['source'] = name\n",
      "/tmp/ipykernel_24719/1047924690.py:23: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  nan_rows_df = nan_rows_df.append(extract_nan_rows(cohere_comments_df, \"tfcc_top_comments_cohere_summarized\"))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_nan_rows(df, name):\n",
    "    nan_rows = df[df['summary'].isna()]\n",
    "    nan_rows['source'] = name\n",
    "    return nan_rows\n",
    "\n",
    "# Load DataFrames\n",
    "df = pd.read_csv('tfcc_submissions_top20_with_sentiment_including_comment_sentiment_and_summaries.csv')\n",
    "comments_df = pd.read_csv('tfcc_top_comments_summarized.csv')\n",
    "pegasus_df = pd.read_csv('tfcc_submissions_top20_pegasus_summaries.csv')\n",
    "pegasus_comments_df = pd.read_csv('tfcc_top_comments_pegasus_summarized.csv')\n",
    "cohere_df = pd.read_csv('tfcc_submissions_top20_cohere_summaries.csv')\n",
    "cohere_comments_df = pd.read_csv('tfcc_top_comments_cohere_summarized.csv')\n",
    "\n",
    "# Extract rows with NaN values in the 'summary' column\n",
    "nan_rows_df = pd.DataFrame()\n",
    "nan_rows_df = nan_rows_df.append(extract_nan_rows(df, \"tfcc_submissions_top20_with_sentiment_including_comment_sentiment_and_summaries\"))\n",
    "nan_rows_df = nan_rows_df.append(extract_nan_rows(comments_df, \"tfcc_top_comments_summarized\"))\n",
    "nan_rows_df = nan_rows_df.append(extract_nan_rows(pegasus_df, \"tfcc_submissions_top20_pegasus_summaries\"))\n",
    "nan_rows_df = nan_rows_df.append(extract_nan_rows(pegasus_comments_df, \"tfcc_top_comments_pegasus_summarized\"))\n",
    "nan_rows_df = nan_rows_df.append(extract_nan_rows(cohere_df, \"tfcc_submissions_top20_cohere_summaries\"))\n",
    "nan_rows_df = nan_rows_df.append(extract_nan_rows(cohere_comments_df, \"tfcc_top_comments_cohere_summarized\"))\n",
    "\n",
    "# Remove rows with NaN values from the original DataFrames\n",
    "df = df.dropna(subset=['summary'])\n",
    "comments_df = comments_df.dropna(subset=['summary'])\n",
    "pegasus_df = pegasus_df.dropna(subset=['summary'])\n",
    "pegasus_comments_df = pegasus_comments_df.dropna(subset=['summary'])\n",
    "cohere_df = cohere_df.dropna(subset=['summary'])\n",
    "cohere_comments_df = cohere_comments_df.dropna(subset=['summary'])\n",
    "\n",
    "# Print the DataFrame containing rows with NaN values in the 'summary' column\n",
    "nan_rows_df.to_csv('summary_errors.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_date</th>\n",
       "      <th>selftext_length</th>\n",
       "      <th>topic</th>\n",
       "      <th>pos_sentiment</th>\n",
       "      <th>neg_sentiment</th>\n",
       "      <th>comments_pos_sentiment</th>\n",
       "      <th>comments_neg_sentiment</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e01rb8</td>\n",
       "      <td>One of my agents actually said what everyone t...</td>\n",
       "      <td>this happened a couple of weeks ago and is bot...</td>\n",
       "      <td>wirwarennamenlos</td>\n",
       "      <td>1553.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>2019-11-22 14:33:25</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498418</td>\n",
       "      <td>0.501582</td>\n",
       "      <td>0.250270</td>\n",
       "      <td>0.749730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfcc_submissions_top20_cohere_summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b4w6n6</td>\n",
       "      <td>Hung up on a customer today</td>\n",
       "      <td>ill give a little bit of background before i g...</td>\n",
       "      <td>forever_a10ne</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2019-03-24 13:11:29</td>\n",
       "      <td>395.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235142</td>\n",
       "      <td>0.764858</td>\n",
       "      <td>0.131666</td>\n",
       "      <td>0.868334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfcc_submissions_top20_cohere_summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cf0t8g</td>\n",
       "      <td>Perv masturbates loudly and the rep documents it</td>\n",
       "      <td>so, i wasn't sure i wanted to put this one her...</td>\n",
       "      <td>TaraJo</td>\n",
       "      <td>920.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2019-07-19 00:48:39</td>\n",
       "      <td>446.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.199540</td>\n",
       "      <td>0.800460</td>\n",
       "      <td>0.392115</td>\n",
       "      <td>0.607885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfcc_submissions_top20_cohere_summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bqz64k</td>\n",
       "      <td>I Love Karma...</td>\n",
       "      <td>this happened to me a few years ago but it sti...</td>\n",
       "      <td>David-Arroyo</td>\n",
       "      <td>756.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2019-05-20 18:17:20</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645398</td>\n",
       "      <td>0.354602</td>\n",
       "      <td>0.269660</td>\n",
       "      <td>0.730340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfcc_submissions_top20_cohere_summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>k60ejc</td>\n",
       "      <td>Your Son is Seven and He's Getting WHAT?</td>\n",
       "      <td>so last night i had a very bizarre call from s...</td>\n",
       "      <td>olivecornbread</td>\n",
       "      <td>553.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2020-12-03 16:25:56</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.422916</td>\n",
       "      <td>0.577084</td>\n",
       "      <td>0.206383</td>\n",
       "      <td>0.793617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfcc_submissions_top20_cohere_summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>55azb0</td>\n",
       "      <td>Worst thing you have said to a customer</td>\n",
       "      <td>me: hello you are through to bla bla bla\\nir: ...</td>\n",
       "      <td>rach-2912</td>\n",
       "      <td>141.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2016-10-01 00:38:15</td>\n",
       "      <td>395.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.200170</td>\n",
       "      <td>0.799830</td>\n",
       "      <td>0.095824</td>\n",
       "      <td>0.904176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfcc_submissions_top20_cohere_summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>24xldx</td>\n",
       "      <td>Craziest thing you have been called by an angr...</td>\n",
       "      <td>customer calls in and reaches a frontline t1 a...</td>\n",
       "      <td>sadiegirl66</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2014-05-07 06:22:59</td>\n",
       "      <td>180.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.199234</td>\n",
       "      <td>0.800766</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.997105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfcc_submissions_top20_cohere_summaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love customers who like to make note of your...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfcc_top_comments_cohere_summarized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I used to work in a call center and I vividly ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfcc_top_comments_cohere_summarized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I've been called a lot of names since moving t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tfcc_top_comments_cohere_summarized</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "3     e01rb8  One of my agents actually said what everyone t...   \n",
       "12    b4w6n6                        Hung up on a customer today   \n",
       "21    cf0t8g   Perv masturbates loudly and the rep documents it   \n",
       "35    bqz64k                                    I Love Karma...   \n",
       "58    k60ejc           Your Son is Seven and He's Getting WHAT?   \n",
       "...      ...                                                ...   \n",
       "1928  55azb0            Worst thing you have said to a customer   \n",
       "1957  24xldx  Craziest thing you have been called by an angr...   \n",
       "57       NaN                                                NaN   \n",
       "65       NaN                                                NaN   \n",
       "83       NaN                                                NaN   \n",
       "\n",
       "                                               selftext            author  \\\n",
       "3     this happened a couple of weeks ago and is bot...  wirwarennamenlos   \n",
       "12    ill give a little bit of background before i g...     forever_a10ne   \n",
       "21    so, i wasn't sure i wanted to put this one her...            TaraJo   \n",
       "35    this happened to me a few years ago but it sti...      David-Arroyo   \n",
       "58    so last night i had a very bizarre call from s...    olivecornbread   \n",
       "...                                                 ...               ...   \n",
       "1928  me: hello you are through to bla bla bla\\nir: ...         rach-2912   \n",
       "1957  customer calls in and reaches a frontline t1 a...       sadiegirl66   \n",
       "57    I love customers who like to make note of your...               NaN   \n",
       "65    I used to work in a call center and I vividly ...               NaN   \n",
       "83    I've been called a lot of names since moving t...               NaN   \n",
       "\n",
       "       score  num_comments         created_date  selftext_length  topic  \\\n",
       "3     1553.0         121.0  2019-11-22 14:33:25            138.0    0.0   \n",
       "12    1128.0          86.0  2019-03-24 13:11:29            395.0    0.0   \n",
       "21     920.0          74.0  2019-07-19 00:48:39            446.0    0.0   \n",
       "35     756.0          58.0  2019-05-20 18:17:20            221.0    0.0   \n",
       "58     553.0          68.0  2020-12-03 16:25:56            232.0    0.0   \n",
       "...      ...           ...                  ...              ...    ...   \n",
       "1928   141.0          69.0  2016-10-01 00:38:15            395.0   19.0   \n",
       "1957    19.0          29.0  2014-05-07 06:22:59            180.0   19.0   \n",
       "57       NaN           NaN                  NaN              NaN   11.0   \n",
       "65       NaN           NaN                  NaN              NaN   13.0   \n",
       "83       NaN           NaN                  NaN              NaN   16.0   \n",
       "\n",
       "      pos_sentiment  neg_sentiment  comments_pos_sentiment  \\\n",
       "3          0.498418       0.501582                0.250270   \n",
       "12         0.235142       0.764858                0.131666   \n",
       "21         0.199540       0.800460                0.392115   \n",
       "35         0.645398       0.354602                0.269660   \n",
       "58         0.422916       0.577084                0.206383   \n",
       "...             ...            ...                     ...   \n",
       "1928       0.200170       0.799830                0.095824   \n",
       "1957       0.199234       0.800766                0.002895   \n",
       "57              NaN            NaN                     NaN   \n",
       "65              NaN            NaN                     NaN   \n",
       "83              NaN            NaN                     NaN   \n",
       "\n",
       "      comments_neg_sentiment summary                                   source  \n",
       "3                   0.749730     NaN  tfcc_submissions_top20_cohere_summaries  \n",
       "12                  0.868334     NaN  tfcc_submissions_top20_cohere_summaries  \n",
       "21                  0.607885     NaN  tfcc_submissions_top20_cohere_summaries  \n",
       "35                  0.730340     NaN  tfcc_submissions_top20_cohere_summaries  \n",
       "58                  0.793617     NaN  tfcc_submissions_top20_cohere_summaries  \n",
       "...                      ...     ...                                      ...  \n",
       "1928                0.904176     NaN  tfcc_submissions_top20_cohere_summaries  \n",
       "1957                0.997105     NaN  tfcc_submissions_top20_cohere_summaries  \n",
       "57                       NaN     NaN      tfcc_top_comments_cohere_summarized  \n",
       "65                       NaN     NaN      tfcc_top_comments_cohere_summarized  \n",
       "83                       NaN     NaN      tfcc_top_comments_cohere_summarized  \n",
       "\n",
       "[84 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_rows_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure summary accuracy and drop all rows that are below 0.8 F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of rows using the shape attribute\n",
    "start_num_rows = df.shape[0]\n",
    "pegasus_start_num_rows = pegasus_df.shape[0]\n",
    "cohere_start_num_rows = cohere_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI - Measuring accuracy on 1964 rows.\n",
      "Pegasus - Measuring accuracy on 1964 rows.\n",
      "COhere - Measuring accuracy on 1883 rows.\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenAI - Measuring accuracy on \" +str(start_num_rows) + \" rows.\")\n",
    "print(\"Pegasus - Measuring accuracy on \" +str(pegasus_start_num_rows) + \" rows.\")\n",
    "print(\"COhere - Measuring accuracy on \" +str(cohere_start_num_rows) + \" rows.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate accuracy for OpenAI submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdffc3f21cb44d8b2ea4f9b4d53c641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a4d731eeba4a11a703cb0dab73ef2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 100.36 seconds, 19.57 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your existing code\n",
    "selftext = df['selftext']\n",
    "summary = df['summary']\n",
    "P, R, F1 = bert_score.score(selftext.tolist(), summary.tolist(), lang='en', verbose=True)\n",
    "df['bert_f1'] = F1.tolist()\n",
    "\n",
    "# Free up GPU memory\n",
    "del P, R, F1\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate accuracy for Pegasus submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca408c334c6846199047d5b4228ea35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb6599083754cf0a7598d0ac0d6be1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 99.14 seconds, 19.81 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "selftext = pegasus_df['selftext']\n",
    "summary = pegasus_df['summary']\n",
    "\n",
    "# Compute the BERTScore for each pair of text\n",
    "P, R, F1 = bert_score.score(selftext.tolist(), summary.tolist(), lang='en', verbose=True)\n",
    "pegasus_df['bert_f1'] = F1.tolist()\n",
    "\n",
    "# Free up GPU memory\n",
    "del P, R, F1\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate accuracy for Cohere submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd2a39b522d4e0bb294d08d6630c1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2c7e3fb02944c2a732512c12cacc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 103.39 seconds, 18.21 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "selftext = cohere_df['selftext']\n",
    "summary = cohere_df['summary']\n",
    "\n",
    "# Compute the BERTScore for each pair of text\n",
    "P, R, F1 = bert_score.score(selftext.tolist(), summary.tolist(), lang='en', verbose=True)\n",
    "cohere_df['bert_f1'] = F1.tolist()\n",
    "\n",
    "# Free up GPU memory\n",
    "del P, R, F1\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display average accuracy score for OpenAI, Pegasus, and Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dataframe</th>\n",
       "      <th>Average F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>df</td>\n",
       "      <td>0.842497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pegasus_df</td>\n",
       "      <td>0.855814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cohere_df</td>\n",
       "      <td>0.869656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Assuming you have already loaded the dataframes: df, pegasus_df, and cohere_df\n",
    "\n",
    "# Calculate the average F1 scores\n",
    "df_avg_f1 = df['bert_f1'].mean()\n",
    "pegasus_avg_f1 = pegasus_df['bert_f1'].mean()\n",
    "cohere_avg_f1 = cohere_df['bert_f1'].mean()\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "data = {\n",
    "    'Dataframe': ['df', 'pegasus_df', 'cohere_df'],\n",
    "    'Average F1': [df_avg_f1, pegasus_avg_f1, cohere_avg_f1]\n",
    "}\n",
    "\n",
    "# Create a new dataframe to display the results\n",
    "results_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the results in a simple table\n",
    "display(HTML(results_df.to_html(index=False)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print results for OpenAI test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1943 rows were >= 0.8 F1 and 21 were below 0.8 F1 and have been removed.\n"
     ]
    }
   ],
   "source": [
    "df2 = df[df['bert_f1'] < 0.8] \n",
    "df = df[df['bert_f1'] >= 0.8]\n",
    "\n",
    "end_num_rows = df.shape[0]\n",
    "\n",
    "removed_num = df2.shape[0]\n",
    "\n",
    "print(str(end_num_rows) + \" rows were >= 0.8 F1 and \" + str(removed_num) + \" were below 0.8 F1 and have been removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tfcc_submissions_with_accuracy.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print results for Pegasus test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950 rows were >= 0.8 F1 and 14 were below 0.8 F1 and have been removed.\n"
     ]
    }
   ],
   "source": [
    "pegasus_df2 = pegasus_df[pegasus_df['bert_f1'] < 0.8] \n",
    "pegasus_df = pegasus_df[pegasus_df['bert_f1'] >= 0.8]\n",
    "\n",
    "end_num_rows = pegasus_df.shape[0]\n",
    "\n",
    "removed_num = pegasus_df2.shape[0]\n",
    "\n",
    "print(str(end_num_rows) + \" rows were >= 0.8 F1 and \" + str(removed_num) + \" were below 0.8 F1 and have been removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_df.to_csv('tfcc_submissions_pegasus_with_accuracy.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print results for Cohere test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849 rows were >= 0.8 F1 and 34 were below 0.8 F1 and have been removed.\n"
     ]
    }
   ],
   "source": [
    "cohere_df2 = cohere_df[cohere_df['bert_f1'] < 0.8] \n",
    "cohere_df = cohere_df[cohere_df['bert_f1'] >= 0.8]\n",
    "\n",
    "end_num_rows = cohere_df.shape[0]\n",
    "\n",
    "removed_num = cohere_df2.shape[0]\n",
    "\n",
    "print(str(end_num_rows) + \" rows were >= 0.8 F1 and \" + str(removed_num) + \" were below 0.8 F1 and have been removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_df.to_csv('tfcc_submissions_cohere_with_accuracy.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat accuracy measurement process for comments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Measuring accuracy on 98 rows.\n",
      "Pegasus Measuring accuracy on 98 rows.\n",
      "Cohere Measuring accuracy on 96 rows.\n"
     ]
    }
   ],
   "source": [
    "# get the number of rows using the shape attribute\n",
    "start_num_rows = comments_df.shape[0]\n",
    "print(\"OpenAI Measuring accuracy on \" +str(start_num_rows) + \" rows.\")\n",
    "\n",
    "pegasus_start_num_rows = pegasus_comments_df.shape[0]\n",
    "print(\"Pegasus Measuring accuracy on \" +str(pegasus_start_num_rows) + \" rows.\")\n",
    "\n",
    "cohere_start_num_rows = cohere_comments_df.shape[0]\n",
    "print(\"Cohere Measuring accuracy on \" +str(cohere_start_num_rows) + \" rows.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure OpenAI comments summaries accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00441da833c49ccb9bde8c8af4bf413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07063010467f4de3b8163b176510c261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7.20 seconds, 13.62 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "# Select the columns to compare\n",
    "selftext = comments_df['selftext']\n",
    "summary = comments_df['summary']\n",
    "\n",
    "# Compute the BERTScore for each pair of text\n",
    "P, R, F1 = bert_score.score(selftext.tolist(), summary.tolist(), lang='en', verbose=True)\n",
    "comments_df['bert_f1'] = F1.tolist()\n",
    "\n",
    "# Free up GPU memory\n",
    "del P, R, F1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure Pegasus comments summaries accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68db853594aa4892afa0f962d8e3f14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e629cc864ddc4025b5cd950d0abf59dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 6.73 seconds, 14.57 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "# Select the columns to compare\n",
    "selftext = pegasus_comments_df['selftext']\n",
    "summary = pegasus_comments_df['summary']\n",
    "\n",
    "# Compute the BERTScore for each pair of text\n",
    "P, R, F1 = bert_score.score(selftext.tolist(), summary.tolist(), lang='en', verbose=True)\n",
    "pegasus_comments_df['bert_f1'] = F1.tolist()\n",
    "\n",
    "# Free up GPU memory\n",
    "del P, R, F1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure Cohere comments summaries accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea44aeb3b813432b92c87d87ae2eb17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ea683999d24a7ebb4fb15681859d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7.67 seconds, 12.52 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "# Select the columns to compare\n",
    "selftext = cohere_comments_df['selftext']\n",
    "summary = cohere_comments_df['summary']\n",
    "\n",
    "# Compute the BERTScore for each pair of text\n",
    "P, R, F1 = bert_score.score(selftext.tolist(), summary.tolist(), lang='en', verbose=True)\n",
    "cohere_comments_df['bert_f1'] = F1.tolist()\n",
    "\n",
    "# Free up GPU memory\n",
    "del P, R, F1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display average accuracy score for OpenAI, Pegasus, and Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dataframe</th>\n",
       "      <th>Average F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>df</td>\n",
       "      <td>0.814839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pegasus_df</td>\n",
       "      <td>0.832171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cohere_df</td>\n",
       "      <td>0.827809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the average F1 scores\n",
    "df_avg_f1 = comments_df['bert_f1'].mean()\n",
    "pegasus_avg_f1 = pegasus_comments_df['bert_f1'].mean()\n",
    "cohere_avg_f1 = cohere_comments_df['bert_f1'].mean()\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "data = {\n",
    "    'Dataframe': ['df', 'pegasus_df', 'cohere_df'],\n",
    "    'Average F1': [df_avg_f1, pegasus_avg_f1, cohere_avg_f1]\n",
    "}\n",
    "\n",
    "# Create a new dataframe to display the results\n",
    "results_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the results in a simple table\n",
    "display(HTML(results_df.to_html(index=False)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print OpenAI results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 rows were >= 0.8 F1 and 1 were below 0.8 F1 and have been removed.\n"
     ]
    }
   ],
   "source": [
    "comments_df2 = comments_df[comments_df['bert_f1'] < 0.8] \n",
    "comments_df = comments_df[comments_df['bert_f1'] >= 0.8]\n",
    "\n",
    "end_num_rows = comments_df.shape[0]\n",
    "\n",
    "removed_num = comments_df2.shape[0]\n",
    "\n",
    "print(str(end_num_rows) + \" rows were >= 0.8 F1 and \" + str(removed_num) + \" were below 0.8 F1 and have been removed.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Pegasus results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 rows were >= 0.8 F1 and 2 were below 0.8 F1 and have been removed.\n"
     ]
    }
   ],
   "source": [
    "pegasus_comments_df2 = pegasus_comments_df[pegasus_comments_df['bert_f1'] < 0.8] \n",
    "pegasus_comments_df = pegasus_comments_df[pegasus_comments_df['bert_f1'] >= 0.8]\n",
    "\n",
    "end_num_rows = pegasus_comments_df.shape[0]\n",
    "\n",
    "removed_num = pegasus_comments_df2.shape[0]\n",
    "\n",
    "print(str(end_num_rows) + \" rows were >= 0.8 F1 and \" + str(removed_num) + \" were below 0.8 F1 and have been removed.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Cohere results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 rows were >= 0.8 F1 and 17 were below 0.8 F1 and have been removed.\n"
     ]
    }
   ],
   "source": [
    "cohere_comments_df2 = cohere_comments_df[cohere_comments_df['bert_f1'] < 0.8] \n",
    "cohere_comments_df = cohere_comments_df[cohere_comments_df['bert_f1'] >= 0.8]\n",
    "\n",
    "end_num_rows = cohere_comments_df.shape[0]\n",
    "\n",
    "removed_num = cohere_comments_df2.shape[0]\n",
    "\n",
    "print(str(end_num_rows) + \" rows were >= 0.8 F1 and \" + str(removed_num) + \" were below 0.8 F1 and have been removed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double check that we still have comments for every top.  Count should = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "unique_count = comments_df['topic'].nunique()\n",
    "\n",
    "print(unique_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegasus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "unique_count = pegasus_comments_df['topic'].nunique()\n",
    "\n",
    "print(unique_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "unique_count = cohere_comments_df['topic'].nunique()\n",
    "\n",
    "print(unique_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export comments with accuracy to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('tfcc_comments_with_accuracy.csv', index=False)\n",
    "pegasus_comments_df.to_csv('tfcc_pegasus_comments_with_accuracy.csv', index=False)\n",
    "cohere_comments_df.to_csv('tfcc_cohere_comments_with_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67cfafb2966ad38f021c3760fdc7d3abae7cbc8d411c26a5ddb66b3636203364"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
